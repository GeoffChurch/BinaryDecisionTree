{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Project 5 - Decision Tree\n",
    "## Jesse Brizzi, Geoffrey Churchill  \n",
    "\n",
    "This is a printout of an IPython Notebook for Project 5.          \n",
    "The original [python project](https://github.com/GeoffChurch/BinaryDecisionTree/) to run this is included with the submssion, and the original IPython notebook file can be downloaded [here](https://github.com/GeoffChurch/BinaryDecisionTree/edit/master/Decision%20Tree.ipynb).\n",
    "\n",
    "### Intro\n",
    "In this project we create a decision tree generator to predict when customers will leave a company's website. This notebook will outline our code structure, walk you through each step of the project, and complete with a detailed analysis of the performance on the provided data.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict,Counter\n",
    "from bisect import bisect_right\n",
    "# numpy and scipy are available from the anaconda collection:\n",
    "#continuum.io/downloads\n",
    "from numpy import log2\n",
    "from scipy.stats import chi2\n",
    "try:\n",
    "\timport cPickle as pickle\n",
    "except:\n",
    "\timport pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store our finished tree as a fractally-nested list. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree=[(23,[3,7]),'0',[(7,[2]),'0','1'],'1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "would represent the following decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify(element):\n",
    "    attrVal = element.getAttr(23)\n",
    "    if attrVal<3:\n",
    "        return 0\n",
    "    elif attrVal<7:\n",
    "        attrVal=element.getAttr(7)\n",
    "        if attrVal<2:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, while the tree is being grown, each leaf is a list of elements, rather than a label ('1' or '0'). For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree=[(23,[3,7]),[('0',[2,4,3,3]),('0',[6,4,1,5]),('1',[1,2,3,4])]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we've finished growing, we simply change each leaf node to the mode of the labels of its elements. This is done using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "sets a node equal to the most common classification label among its elements\n",
    "\"\"\"\n",
    "def setClassifiers(parent,index):\n",
    "\tnode=parent[index]\n",
    "\tif isLeaf(node):\n",
    "\t\tparent[index]=Counter(element[0] for element in node).most_common(1)[0][0]\n",
    "\telse:\n",
    "\t\tfor index in xrange(len(node)-1):\n",
    "\t\t\tsetClassifiers(node,index+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core of our implementation is the following two methods, grow and split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "given a pointer to a node, recursively splits it until its entropy is below the maximum allowed entropy\n",
    "\"\"\"\n",
    "def grow(self,parent,pIndex):\n",
    "\tnode=parent[pIndex]\n",
    "\tif isLeaf(node): # if it's a leaf node, try to split it\n",
    "\t\tentropy=getEntropy(node)\n",
    "\t\tif(entropy>self.maxEntropy):\n",
    "\t\t\tself.split(parent,pIndex,entropy)\n",
    "\t\telse:\n",
    "\t\t\tprint 'leaf entropy is low enough:',str(entropy),'<=',str(self.maxEntropy)\n",
    "\telse: # if it's an internal node, recurse on all children\n",
    "\t\tfor index in xrange(len(node)-1):\n",
    "\t\t\tself.grow(node,index+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "given a pointer to a node, and its current entropy, attempts to split it on the best possible attribute and value.\n",
    "the current implementation does a binary split, but this can easily be changed to support n-ary splits\n",
    "\"\"\"\n",
    "def split(self,parent,pIndex,minEntropy):\n",
    "\tnode=parent[pIndex]\n",
    "\tprint '\\nAttempting to split leaf (size:',len(node),'entropy:'+str(minEntropy)+')'\n",
    "\tminTup=None\n",
    "\tfor attr in xrange(len(node[0][1])): # for each attribute\n",
    "\t\tnode.sort(key=lambda sample:sample[1][attr]) # sort the elements on that attribute\n",
    "\t\tcurrVal=node[0][1][attr]\n",
    "\t\tfor index,(_,sample) in enumerate(node):\n",
    "\t\t\tnewVal=sample[attr]\n",
    "\t\t\tif not newVal is currVal: # and every time the attribute's value changes we simulate a split and determine the subsequent total entropy\n",
    "\t\t\t\tcurrVal=newVal\n",
    "\t\t\t\tcurrEntropy,lChild,rChild=entropyBelow(minEntropy,node,attr,sample[attr],index)\n",
    "\t\t\t\tif currEntropy<minEntropy: # if the new entropy is lower than our previous min, save the split\n",
    "\t\t\t\t\tprint 'split on attribute',self.attrNames[attr] if self.attrNames else attr,'at value',str(sample[attr])+': entropy='+str(currEntropy)\n",
    "\t\t\t\t\tminEntropy=currEntropy\n",
    "\t\t\t\t\tminTup=(attr,sample[attr],lChild,rChild)\n",
    "\tif not minTup:\n",
    "\t\tprint 'Leaf is homogenous with value',map(lambda (attr,val): self.attrNames[attr]+': '+str(val),enumerate(node[0][1])) if self.attrNames else node[0][1],'and entropy',minEntropy\n",
    "\t\treturn\n",
    "\tminAttr,splitpoint,leftChild,rightChild=minTup\n",
    "\tprint 'size of children:'\n",
    "\tprint '\\tleft:',len(leftChild)\n",
    "\tprint '\\tright:',len(rightChild)\n",
    "\tpval=pvalue(node,[leftChild,rightChild])\n",
    "\tprint 'pValue:',pval\n",
    "\tif pval>=self.maxP: # if our split isn't likely to lead to another statistically significant difference, we give up\n",
    "\t\tprint 'Stop splitting!'\n",
    "\telse: # otherwise, perform the actual split and recurse\n",
    "\t\tparent[pIndex]=[(minAttr,[splitpoint]),leftChild,rightChild]\n",
    "\t\tself.grow(parent,pIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine when and when not to split, we use entropy and p-value measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current entropy of a node, used by grow, is calculated by getEntropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "determines the entropy of elements with respect to their classification labels\n",
    "\"\"\"\n",
    "def getEntropy(elements):\n",
    "\tsize=float(len(elements))\n",
    "\tcounter=Counter(element[0] for element in elements)\n",
    "\tentropy=0\n",
    "\tfor _,val in counter.iteritems():\n",
    "\t\tprob=val/size\n",
    "\t\tentropy-=prob*log2(prob)\n",
    "\treturn entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entropy of a node after simulating a split, used by...split, is calculated by entropyBelow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "determines the entropy of the specified *SORTED* node after undergoing a split\n",
    "on attrIndex at the value attrVal.\n",
    "\"\"\"\n",
    "def entropyBelow(maxEntropy,elements,attrIndex,attrVal,splitIndex):\n",
    "\ttotal=float(len(elements))\n",
    "\t#elements is already sorted so we just search for the first occurrence\n",
    "\tsplitIndex=binaryFirstOccurence(attrIndex,attrVal,elements,0,splitIndex)\n",
    "\t#get left-child entropy\n",
    "\tsubseqL=elements[:splitIndex]\n",
    "\tsubtotal=len(subseqL)\n",
    "\tsize=float(subtotal)\n",
    "\tcounter=Counter([element[0] for element in subseqL])\n",
    "\tlEntropy=0\n",
    "\tfor (_,val) in counter.iteritems():\n",
    "\t\tprob=val/size\n",
    "\t\tlEntropy-=prob*log2(prob)\n",
    "\tlEntropy*=subtotal/total\n",
    "\tif lEntropy>=maxEntropy: # if we've already surpassed maxEntropy, we fail early\n",
    "\t\treturn (lEntropy,None,None)\n",
    "\t#get right-child entropy\n",
    "\tsubseqR=elements[splitIndex:]\n",
    "\tsubtotal=len(subseqR)\n",
    "\tsize=float(subtotal)\n",
    "\tcounter=Counter([element[0] for element in subseqR])\n",
    "\trEntropy=0\n",
    "\tfor (_,val) in counter.iteritems():\n",
    "\t\tprob=val/size\n",
    "\t\trEntropy-=prob*log2(prob)\n",
    "\treturn ((rEntropy*subtotal/total)+lEntropy,subseqL,subseqR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The p-value of a split is calculated by pvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "derives the p-value of splitting parent into leaves using the chi-squared test\n",
    "\"\"\"\n",
    "def pvalue(parent,leaves):\n",
    "\t#derive chi-squared statistic\n",
    "\tchiT=0\n",
    "\tn=float(len(parent))\n",
    "\tparentCounter=Counter(element[0] for element in parent)\n",
    "\tfor leaf in leaves:\n",
    "\t\tratio=len(leaf)/n\n",
    "\t\tfor attr,leafCount in Counter(element[0] for element in leaf).iteritems():\n",
    "\t\t\texpected=parentCounter[attr]*ratio\n",
    "\t\t\tchiT+=((expected-leafCount)**2)/expected\n",
    "\tprint 'chiT:',chiT\n",
    "\t#derive p-value\n",
    "\treturn chi2.sf(chiT,len(parentCounter)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Results\n",
    "For these results, we've kept the maxEntropy paramenter constant at 1, to demonstrate the effects of maxP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "####maxP=1.00\n",
    "internal nodes: 8118<br>\n",
    "leaf nodes: 8119<br>\n",
    "total: 16237<br>\n",
    "performance: 0.63352"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "####maxP=0.05\n",
    "internal nodes: 337<br>\n",
    "leaf nodes: 338<br>\n",
    "total: 675<br>\n",
    "performance: 0.71156"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "####maxP=0.01\n",
    "internal nodes: 152<br>\n",
    "leaf nodes: 153<br>\n",
    "total: 305<br>\n",
    "performance: 0.72572"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "####maxP=0.001\n",
    "internal nodes: 78<br>\n",
    "leaf nodes: 79<br>\n",
    "total: 157<br>\n",
    "performance: 0.73404"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "####maxP=0.0001\n",
    "internal nodes: 46<br>\n",
    "leaf nodes: 47<br>\n",
    "total: 93<br>\n",
    "performance: 0.72824"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Conclusion\n",
    "When maxP is equal to 1, we don't stop splitting until every node is homogenous either in classifications or attributes. Because each node ends up with only a few elements, we lose some of the \"big picture\" of the data we're trying to classify, which leads to overfitting.<br>\n",
    "Lowering maxP means that we'll only split a node if the split would result in a highly unexpected (non-trivial) distribution. The resultant nodes are more populous, so we have a clearer picture of the macro features of our data. Changing the p-value threshold is akin to focusing a lens to compromise between details (foreground) and patterns (background).<br>\n",
    "<br>\n",
    "We found that the tree performed best when maxP was about 0.001, which indicates that that is the appropriate level of generality for this dataset. Below that, the performance began to drop, indicating the complement of overfitting: underfitting. Underfitting occurs when we ignore aspects of the data that would have helped us in general.<br>\n",
    "<br>\n",
    "Because we prune our tree as we grow it, we're heuristically narrowing our search space to a very small subset based on naive probability measurements. Our tree would be more performant if we instead grew it completely and then pruned away extraneous nodes starting from the bottom, because we'd be pruning over the whole tree, rather than on a node-by-node basis, and because our judge of whether a branch is worthwhile would be based on its actual performance, rather than on how surprising it was.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
